# Approaches to Anomaly Detection

## Model-Based Techniques
- A model can be clusters, linear regression, etc. If clusters, the anomaly is an instance which does not strongly belong to any classes.

## Proximity-Based Techniques
- We use distance-based outlier detection.

## Density-Based Techniques
- We use density of the data, and this approach is similar to proximity-based approaches since we also use distance metrics.

# Detecting Outliers in a Univariate Normal Distribution
- We can create a Gaussian distribution from the data and examine data with low probability as candidate anomalies. To do that we can transform x into z (z-score) where `z = (x - mean) / std`.

# Outliers in a Multivariate Normal Distribution
- We can get distance between x and mean of the data. An example distance metric for this purpose is Mahalonobis distance
- `mahalonobis(x, x_mean) = (x - x_mean) S^-1 (x - x_mean)^T`

- A nice example with Gaussian distribution is [here](https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b).

- Statistical approaches may work poorly on multi-dimensional data.

# K-nearest Neighbors
- We can mark items as outliers if their distance to their k-nearest neighbors is high. This approach is very sensitive to input `k`. The complexity is O(m^2) and it is expensive for large data sets.

# Clustering-Based Techniques
- First approach is to discard small clusters which are far from other clusters. This approach is sensitive to the number of clusters and the distance threshold.
- Another approach is to cluster the items first, and then check each items distance to its cluster center.
- We can simply calculate the distance from the instance to the nearest centroid. An other option is to calculate distance and divide it with the median distance of the cluster instances. The second formula is less sensitive to cluster density. Because cluster density can be compact or loose.

## Impact of Outliers to the Initial Clustering
- One approach is the cluster all objects, then remove outliers and cluster again.
- Another approach is to put potential outliers to a cluster at the beginning. After clustering the remaining items will be labeled as outliers.

# Evaluation
- AUC is one of the most common evaluation metrics for anomaly detection. We can also check confusion matrix.


# GaussianMixture

## Univariate Gaussian Mixture

- Mixture models can cluster points better than K-means when we have ellipsoidal clusters.


## Expectation Minimization

```
Select inital set of model parameters (e.g. by using Kmeans)

repeat
  Expectation step: For each instance calculate the probability of the object belongs to each distribution. prob(distribution j | xi, O)
  Maximization step: Given the probabilities, find the new estimates of the parameters of that maximize the expected likelihood. (Assigning the points to the new clusters)
until The parameters do not change or changes under a threshold.

```

- `prob(distribution j | xi, O)`: We use Gaussian distribution formula to calculate this. So, we need mean, and std of distribution j. If the data has more than one dimension then we will need covariance matrix, instead of the std.

- `prob(distribution j | xi, O)` is calculated by using Bayes Rule, so first we calculate `prob(xi, Oj)` for each `O` and then we use Bayes Rule to calculate `prob(distribution j | xi, O)`. Because sum of `prob(distribution j | xi, Oj)` should be 1.

- Since probability score with Gaussian distribution will be very small, we can use log probability.

```

from sklearn.mixture import GaussianMixture
gm = GaussianMixture(n_components = 5, covariance_type = 'full', random_state=0, )
gm.fit(X_train)
gm.predict_proba(X_train)

```

- `predict_proba` returns the probability of the instance to belongs to the clusters. Each instance has 5 probabilities in this example.

- **Swamping**: Labeling normal events as anomalies.
- **Masking**: Labeling anomalies as normal events.
- **Normalization**: Rescaling the data into a range of [0, 1]. Hides the outliers.
- **Standardization**: Rescaling the data to have a mean of 0 and a standard deviation of 1. ([-1, 1]). Does not hide the outliers.
- K-means is sensitive to outliers. It is better to use Gaussian Mixture.
- **Type of anomalies**:
  - Point anomalies: Single instance is anomalous.
  - Contextual anomalies: The instance is anomalous in a specific context. e.g. 1000$ is normal for a bank account, but it is not normal for a student.
  - Collective anomalies: A set of instances is anomalous. e.g. heart beat stops for awhile.
- **Outlier detection apporaches**:
  - Model-based: We assume that normal instances are generated by a model. We can use Gaussian Mixture for this purpose.
  - Proximity-based: We assume that normal instances are close to each other. We can use K-nearest neighbors for this purpose.
  - Density-based: We assume that normal instances are dense. We can use DBSCAN for this purpose.
  - Clustering-based: We assume that normal instances belong to the same clusters. We can use DBSCAN for this purpose.
- **Unsupervised Anamonaly Detection**:
  - Local Outlier Factor (LOF): It measures the local density of the instances. If a point's neighbors are far away from it compared to the point's neighbors, it is an anomaly.
  - DBSCAN: It measures the density of the instances. If an instance has few neighbors, it is an anomaly.
  - Isolation Forest: It isolates instances by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. It is good for high-dimensional data.
  - Gaussian Mixture: It assumes that normal instances belong to a small number of clusters. It is good for low-dimensional data.
  - Prophet: It is a time series forecasting model. It is good for time series data. It uses trend, seasonality and holidays.
- **One Class Classification**:
  - **Autoencoders:** It is a neural network that tries to reproduce its inputs. If the error is high, it is an anomaly.
    - Input -> Encoder -> Latent Space -> Decoder -> Output 
    - Input and output layers have the same number of neurons.
    - Loss function is the mean squared error between the input and the output.
    - Reconstruction error is the difference between the input and the output. If the error is higher than a threshold, it is an anomaly.
- **Challenges in Anomaly Detection projects:**
  - Lack of labeled data.
  - Data pre-processing and feature engineering.
  - Selecting appropriate algorithm.
  - Concept drift.
  - Minimizing false positives.
- **Handling false positives and false negatives:**
  - Adjusting the threshold.
  - Feature engineering.
  - Ensemble methods.