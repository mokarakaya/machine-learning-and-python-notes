# Given
# Define the problem
- What is the use case and the requirements?
- What are success criteria?
  - Technical metrics.
  - KPI.
# Design the data processing pipeline
- What are the data sources?
- How do we collect data and maintain features?
# Create a model architecture
- One simple model.
- One complex model.
# Train the model
- How do we collect training dataset?
  - Ensure the dataset is representative.
    - Stratified sampling.
    - Standard error of the features etc.
- How do we split the dataset?
  - Train, validation, test. 
  - Cross-validation.
- How do we train the model?
  - Loss function.
  - Optimizer.
  - Regularization.
  - Early stopping.
  - Learning rate scheduling.
  - Data augmentation.
  - Batch normalization.
  - Dropout.
  - Weight initialization.
  - Gradient clipping.
  - Hyperparameter tuning.
# Evaluate the model
- Evaluation metrics.
- Robustness.
- Explainability.
- Cost.
- Latency.
- Memory requirements.
# Deploy and monitor the model
- How do we deploy the model?
  - Integration tests.
  - Model serving.
  - Model versioning.
  - Model monitoring.
- How do we monitor the model?
  - Latency.
  - Throughput.
  - Error rates.
  - Resource usage.
  - Concept drift.
  - Data drift.
- How do we ensure the model is backward compatible?
- How do we scale the model?
  - Performance test to find bottlenecks.
  - Horizontal scaling.
  - Vertical scaling.
  - Auto-scaling.
  - Rate limiting.
  - Throttling.
  - Request batching.
  - Adding requests to queue.
- How do we close the feedback loop?
  - Train from scratch vs fine-tuning.
# Wrap up 

