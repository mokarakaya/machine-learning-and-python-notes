- **Pros**: High accuracy, insensitive to outliers, no assumptions about data.
- **Cons**: Computationally expensive, requires a lot of memory.


```
For every point in our dataset:
  calculate the distance between inX and the current point
  sort the distances in increasing order
  take k items with lowest distances to inX
  find the majority class among these items
  return the majority class as our prediction for the class of inX
```
- kNN is a instance-based learning algorithm. The complexity of classifying a new instance is O(n).

- Scaling helps knn to work better.
- We can also use knn for imputing.
- We can use `Hamming Distance` for categorical variables. It calculates the number of different bits between two features.
- `Manhattan Distance` is an example to calculate continuous variables. It calculates the sum of absolute distance for each variables. If `a = x1, y1`, and `b = x2, y2` then distance between a, b is `|x1 - x2| + |y1 - y2|`
- Defining `k` as an odd number is better since we get the majority of the classes.
- There are several ways to select `k`. Low values of `k` can be affected by noise easily and high values are computationally expensive.
  - We can use cross validation for different values of `k`.
  - We can get `sqrt(n)` where n is the number of instances in the data set.
  - We can use elbow method where axis y is within-cluster sum of square (`WSS`) and axis x is k.
