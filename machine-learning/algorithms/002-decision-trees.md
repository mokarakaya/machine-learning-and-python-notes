- **Pros**: Computationally cheap to use, easy for humans to understand learned results, missing values OK, can deal with irrelevant features.

- **Cons**: Prone to overfitting.

# ID3 Algorithm
- Pseudo-code for a function called `createBranch()` would look like this:
```
Check if every item in the dataset is in the same class:

  If so return the class label

  Else

    find the best feature to split the data

    split the dataset

    create a branch node

    for each split

      call createBranch and add the result to the branch node

    return branch node
```
- ID3 algorithm runs until there is no items to split, or all items are in the same class.

# Information Gain and Entropy

- In order to find the best split, we need to calculate the information gain and select the highest information gain among the possible splits.

- `H = - \sum_{classes} p(x_i) * log_2 p(x_i)` returns the current entropy of the dataset.
- Casting a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability (about p=1/6) than each outcome of a coin toss (p=1/2).

# Choosing the Best Feature to Split

```
for each feature in dataset:
  for each unique values of the feature:
    subDataSet = split the data set in the feature and value
    prob = len(subDataSet) / len(dataset)
    newEntropy +=  prob * entropy(subDataset)

  infoGain = H - newEntropy
  if infoGain > bestInfoGain
    bestInfoGain = infoGain
```

# Complexity

- `d` number of attributes
- `m` training instances
- `depth` assumed to be O(log m)
- `complexity = O(d m log m)`

# Purity
- It is better to split on the pure subsets. As an example if classes as yes/no and one possible split is 10/0 and the other one is 5/5, it is better to split on 10/0. Because it is pure. x/0 splits are called pure splits.
- Entropy of a pure split is `0`.
- Entropy of a half/half split is `1`.
- `Information Gain = CurrentEntropy - NewEntropy`

# Gain Ratio
- Some algorithms e.g. CART force binary splits even if the feature has more than two classes.
- In some other algorithms e.g. C4.5, it is possible to split into more than 2 nodes.

# Handling Overfitting in Decision Trees

## Prepruning
- Stop expanding the tree when observed gain in impurity measure (or improvement in the estimated generalization error) falls below a certain threshold. It is difficult to choose right threshold.

## Postpruning
- A subtree is replaces in two ways
  - Replace subtree with the majority class in the subtree.
  - Replace subtree with the most frequently used branch.
- Postpruning tends to give better results than prepruning since decisions are made on fully grown tree.

# Random Forest

- Random forests ensemble many decision trees.
- We can create different decision trees by randomly sampling the dataset, or randomly using a subset of the features.
- We lose interpretability by using random-forests.

# AdaBoost
- Adaboost is an ensemble algorithm for decision trees. It is generally used for binary classification.
- AdaBoost combines a lot of week learning to make classification. Most of the times these learners are stumps. Stumps are one-node trees, and they use a single feature for classification.
- Stumps have weights and the weights of the stumps affect final classification.
- Each stump takes the mistakes of the previous stump into account. We assign weights to instances and the dataset is generated by using the weights. If the previous learner had error on an instance, the weight of the instance is higher.
- A neat explanation is [here](https://www.youtube.com/watch?v=LsK-xG1cLYA).

# Gradient Boosting

- We can use Gradient Boosting for both regression and classification.

- Gradient boosting is similar to AdaBoost but a particular tree can go larger than a stump (a tree with single node) e.g. number of leaves between 8 and 32.

- For classification, we create residuals for each instance. Initially they are set as `mean - yi`. Then, we create a tree by using residuals as y. Each instance will end up on a leaf, and for each leaf we get the average of the residuals, and update them as follows `mean + learning_rate * leaf_average`

- A neat explanation is [here](https://www.youtube.com/watch?v=3CC4N4z3GJc)
