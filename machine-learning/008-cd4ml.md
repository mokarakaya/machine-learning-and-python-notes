- There 3 axis of change in a Machine Learning application. These are `data`, `model`, `code`.

# Common Challenges
- Organizational structure: Data engineers, data scientists, and ML developers contribute to the process, and they need to collaborate.
- Process should be reproducible and auditable: We need to increase automation.

# Discoverable and Accessible Data

- We can keep data in a DataLake which is centralized. Another way is to use decentralized Data Mesh architecture.

# Reproducible Model Training
- We can create versions for data and the model by using DVC (Data Version Control). DVC is like git, but it can keep large amounts. It is integrated with git, s3, google drive etc.

# Model Serving
- There are 3 approaches
  - `Embedded Model`: Model, and consuming application are packaged together. `MLeap` provides a common serialization format for exporting/importing Spark, scikit-learn and Tensorflow models.
  - `Model deployed as a separate service`: Model wrapped in a service and served separately from the consuming application. We need to call the service from the consuming application.
  - `Model published as data`: Model is also treated and published independently, but the consuming application will ingest it as data at runtime.


# Testing and Quality in Machine Learning

- There are several types of tests in ML workflow;
  - `Validating data`: Data is in expected range, not null, scaled, one-hot encoding is correct, etc.
  - `Validating component integration`: Integration between the model service, and the consuming application is correct. We can use contract based testing for this purpose.
  - `Validating the model quality`: Performance of the model (e.g. accuracy) is not below a threshold.
  - `Validating model bias and fairness`: We can test our model against different slices of data. As an example; the model may not be working well when values in feature X is low. `Facets` can help to visualize slices in the data, and the distributions of the values across the features.


# Experiments Tracking

- We need to run several tests in parallel to find the best model e.g. different algorithms with different hyperparameters, apply tf-idf first or not etc.
- One way is to use Git branches with DVC. Since we do not need to merge all these branches into master, it is fine to use branches.
- `Mlflow` is another experiment tracking tool. It has an API and web interface.

# Model Deployment
- `Multiple models`: We may have multiple models e.g. model1 generates predictions for product type A, and model2 generates predictions for product type B. Then, it is better to create separate services for each model, and deploy separately.

- `Shadow models`: We can deploy a new model without removing the existing and replicate the requests to the new model in order to monitor the performance of the candidate model.

- `Competing models`: A/B testing. However, we need to be careful if modelA, and modelB gets the correct request. E.g. the model B may need to get requests from a subset of customers, or 10% of the customer etc.

- `Online learning models`: We use production data to train existing model on production. So, we need to version production data as well.

# Continuous Delivery Orchestration

- Orchestration tools can combine different pipelines e.g. model training pipeline and application deployment pipeline. `GoCD` is one of the examples.

# Model Monitoring and Observability
- We need to understand how our model is performing on production. To do it, we need to aggregate logs, and metrics. We can use them to evaluate performance.

- `Model inputs`: What is the input/output on production.
- `Model interpretability outputs`: We can use metrics such as `LIME` to investigate the behavior of the model. The output of LIME is a list of explanations, reflecting the contribution of each feature to the prediction of a data sample. Tools like `ELI5` can measure this and allows to visualize and debug various types of models.
- `Model outputs and decisions`: Sometimes the application might choose to ignore the model and make a decision based on pre-defined rules.
- `Model fairness`: Analysing input data and output predictions against known features that could bias, such as race, gender, age, income groups etc.

# End-to-End CD4ML Process

- `Model Building`: Code -> training code, Data -> training data, Model -> candidate models.
- `Model Evaluation and Experimentation`: Data -> test data, metrics, Model -> chosen model.
- `Productionize model`: Model -> productionized model.
- `Testing`: Data -> test data, Code -> test code, Model -> model.
- `Deployment`: Code -> application code, Code + Model -> code and model in production.
- `Monitoring and Observability`: Data -> production data.
